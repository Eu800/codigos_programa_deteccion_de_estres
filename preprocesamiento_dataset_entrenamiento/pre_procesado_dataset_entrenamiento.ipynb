{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerias a utilizar---------------------------------------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.signal import butter, filtfilt\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#FUNCIONES IMPLEMENTADAS------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Lectura de datos %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "def separar_datos(linea):\n",
    "  \"\"\"\n",
    "  Separa una línea del archivo de texto en tres valores de columna.\n",
    "\n",
    "  Argumentos:\n",
    "    linea: Cadena de texto que representa una línea del archivo.\n",
    "\n",
    "  Devuelve:\n",
    "    Tupla con los valores para las tres columnas o `None` si no hay coincidencia.\n",
    "  \"\"\"\n",
    "  match = re.match(r'(\\d+)\\t0\\t(\\d+)\\t(\\d+)', linea)\n",
    "  if match:\n",
    "    return (match.group(1), match.group(2), match.group(3))\n",
    "  else:\n",
    "    return None\n",
    "\n",
    "def leer_datos_txt(ruta_archivo):\n",
    "  \"\"\"\n",
    "  Lee el contenido de un archivo de texto y lo devuelve como un DataFrame de Pandas con tres columnas.\n",
    "\n",
    "  Argumentos:\n",
    "    ruta_archivo: Ruta completa del archivo a leer.\n",
    "\n",
    "  Devuelve:\n",
    "    Un DataFrame de Pandas con tres columnas o `None` si hay un error.\n",
    "  \"\"\"\n",
    "  if not os.path.exists(ruta_archivo):\n",
    "    print(f\"Error: El archivo '{ruta_archivo}' no existe.\")\n",
    "    return None\n",
    "\n",
    "  datos = []\n",
    "  with open(ruta_archivo, 'r') as f:\n",
    "    for linea in f:\n",
    "      valores_columna = separar_datos(linea)\n",
    "      if valores_columna:\n",
    "        datos.append(valores_columna)\n",
    "\n",
    "  df = pd.DataFrame(datos, columns=['# de muestra', 'BVP', 'ECG'])\n",
    "  return df\n",
    "\n",
    "def leer_datos_csv(ruta_archivo):\n",
    "  \"\"\"\n",
    "  Lee el contenido de un archivo CSV y lo devuelve como un DataFrame de Pandas.\n",
    "\n",
    "  Argumentos:\n",
    "    ruta_archivo: Ruta completa del archivo a leer.\n",
    "\n",
    "  Devuelve:\n",
    "    Un DataFrame de Pandas con una columna adicional de numeración.\n",
    "  \"\"\"\n",
    "  if not os.path.exists(ruta_archivo):\n",
    "    print(f\"Error: El archivo '{ruta_archivo}' no existe.\")\n",
    "    return None\n",
    "\n",
    "  df = pd.read_csv(ruta_archivo, header=None)\n",
    "  df.columns = ['BVP', 'ECG']\n",
    "  df.insert(0, '# de muestra', range(1, len(df) + 1))\n",
    "  return df\n",
    "\n",
    "def lectura_datos(ruta_archivo):\n",
    "  \"\"\"\n",
    "  Lee el contenido de un archivo de texto o CSV y lo devuelve como un DataFrame de Pandas con tres columnas.\n",
    "  Luego escala los valores de la columna '# de muestra'.\n",
    "\n",
    "  Argumentos:\n",
    "    ruta_archivo: Ruta completa del archivo a leer.\n",
    "    valor_a_escalar: Valor para escalar la columna '# de muestra'.\n",
    "\n",
    "  Devuelve:\n",
    "    DataFrame de Pandas con la columna '# de muestra' escalada.\n",
    "  \"\"\"\n",
    "  ext = os.path.splitext(ruta_archivo)[1].lower()\n",
    "  if ext == '.txt':\n",
    "    df = leer_datos_txt(ruta_archivo)\n",
    "  elif ext == '.csv':\n",
    "    df = leer_datos_csv(ruta_archivo)\n",
    "  else:\n",
    "    print(f\"Error: El archivo '{ruta_archivo}' tiene una extensión no soportada.\")\n",
    "    return None\n",
    "\n",
    "  return df\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "#Convertir a enteros los datos %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "def convertir_columnas_a_enteros(df):\n",
    "  # Convertir las columnas a números enteros\n",
    "  df['# de muestra'] = pd.to_numeric(df['# de muestra'])\n",
    "  df['ECG'] = pd.to_numeric(df['ECG'])\n",
    "  df['BVP'] = pd.to_numeric(df['BVP'])\n",
    "  return df\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "#Convertir muestras a tiempo en segundos %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "def convertir_muestras_a_tiempo(df):\n",
    "  # Frecuencia de muestreo\n",
    "  sample_rate=1000\n",
    "  \n",
    "  # Crear el eje de tiempo en segundos para las columnas BVP y ECG\n",
    "  time = np.arange(len(df['# de muestra'])) / sample_rate\n",
    "\n",
    "  # Agregar el eje de tiempo al DataFrame\n",
    "  df['Time (s)'] = time\n",
    "\n",
    "  # Tiempo maximo que se le puede dar al rango de graficacion\n",
    "  max_value_time = df['Time (s)'].max()\n",
    "  \n",
    "  return df, max_value_time\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "#Normalizacion de datos %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "def normalizar_datos(df):\n",
    "  # Normalización Min-Max\n",
    "  scaler_min_max = MinMaxScaler()\n",
    "  df[['BVP_minmax', 'ECG_minmax']] = scaler_min_max.fit_transform(df[['BVP', 'ECG']])\n",
    "\n",
    "  #Crear un nuevo dataframe con los datos BVP y ECG filtrados-----------------------\n",
    "  df_filtered = df[['BVP_minmax', 'ECG_minmax', 'Time (s)']].copy()\n",
    "  return df_filtered\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "#filtrado de datos %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "# Filtrado PAM-TOMPKINS------------------------------------------\n",
    "# Filtro paso alto para eliminar la línea de base\n",
    "# Filtro de derivación\n",
    "# Filtro paso bajo\n",
    "# Detectar picos R\n",
    "#----------------------------------------------------------------\n",
    "\n",
    "# Filtro Paso alto-----------------------------------------------\n",
    "def butter_highpass(cutoff, fs, order):\n",
    "  nyq = 0.5 * fs\n",
    "  normal_cutoff = cutoff / nyq\n",
    "  b, a = butter(order, normal_cutoff, btype='high', analog=False)\n",
    "  return b, a\n",
    "\n",
    "def highpass_filter(data, cutoff, fs, order):\n",
    "  b, a = butter_highpass(cutoff, fs, order=order)\n",
    "  y = filtfilt(b, a, data)\n",
    "  return y\n",
    "\n",
    "# Filtro paso bajo-----------------------------------------------\n",
    "def butter_lowpass(cutoff, fs, order):\n",
    "  nyq = 0.5 * fs\n",
    "  normal_cutoff = cutoff / nyq\n",
    "  b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "  return b, a\n",
    "\n",
    "def lowpass_filter(data, cutoff, fs, order):\n",
    "  b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "  y = filtfilt(b, a, data)\n",
    "  return y\n",
    "\n",
    "# Filtro de derivación -------------------------------------------\n",
    "def derivative_filter(data, fs):\n",
    "  diff = np.diff(data)\n",
    "  diff = np.append(diff, 0)  # Añadir un 0 al final para mantener el mismo tamaño\n",
    "  return diff * fs\n",
    "\n",
    "def senial_ECG_pam_tompkins(df, order, cutoff_high, cutoff_low):\n",
    "\n",
    "  fs = 1 / (df.iloc[1, 2] - df.iloc[0, 2])  # frecuencia de muestreo\n",
    "\n",
    "  # Aplicar el filtro pasa alto a las señales ECG\n",
    "  df['ECG_filtered'] = highpass_filter(df['ECG_minmax'], cutoff_high, fs, order)\n",
    "\n",
    "  # Aplicar el filtro de derivación a las señales filtradas\n",
    "  df['ECG_derivada'] = derivative_filter(df['ECG_filtered'], fs)\n",
    "\n",
    "  # Aplicar el filtro pasa bajo a la señal derivada\n",
    "  df['ECG_filtered_minmax'] = lowpass_filter(df['ECG_derivada'], cutoff_low, fs, order)\n",
    "\n",
    "  # Crear un nuevo dataframe con los datos ECG filtrados y derivados\n",
    "  df_filtered = df[['ECG_filtered_minmax', 'Time (s)']].copy()\n",
    "\n",
    "  return df_filtered\n",
    "\n",
    "def senial_BVP_paso_bajo(df, order, cutoff):\n",
    "  \n",
    "  fs = 1 / (df.iloc[1, 2] - df.iloc[0, 2])  # frecuencia de muestreo\n",
    "\n",
    "  # Aplicar el filtro a las señales BVP\n",
    "  df['BVP_filtered_minmax'] = lowpass_filter(df['BVP_minmax'], cutoff, fs, order)\n",
    "\n",
    "  #Crear un nuevo dataframe con los datos BVP filtrados-----------------------\n",
    "  df_filtered = df[['BVP_filtered_minmax', 'Time (s)']].copy()\n",
    "\n",
    "  return df_filtered\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "#Señal ECG y BVP Picos %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "def RR_interval(df_subset, tiempo_ini, tiempo_fin, prominence_ECG,height_ECG,distance_ECG):\n",
    "  # Seleccionar las muestras que se deseen graficar (entre un tiempo inicial y tiempo final en '# de muestra')\n",
    "  df = df_subset[(df_subset['Time (s)'] >= tiempo_ini) & (df_subset['Time (s)'] <= tiempo_fin)]\n",
    "\n",
    "  # Encontrar los picos en el subconjunto de datos\n",
    "  peaks_ecg_subset, _ = find_peaks(df['ECG_filtered_minmax'], prominence=prominence_ECG, height=np.max(df['ECG_filtered_minmax']) * height_ECG, distance=distance_ECG)\n",
    "\n",
    "  # Crear la figura con Plotly\n",
    "  fig = go.Figure()\n",
    "\n",
    "  # Graficar las muestras de ECG en el rango seleccionado\n",
    "  fig.add_trace(go.Scatter(x=df['Time (s)'], y=df['ECG_filtered_minmax'], mode='lines', name='Tiempo (s) vs mV', line=dict(width=0.5)))\n",
    "  fig.add_trace(go.Scatter(x=df['Time (s)'].iloc[peaks_ecg_subset], y=df['ECG_filtered_minmax'].iloc[peaks_ecg_subset], mode='markers', name='Picos más altos', marker=dict(size=5)))\n",
    "\n",
    "  # Calcular las distancias entre todos los picos\n",
    "  rr_intervals = []\n",
    "  if len(peaks_ecg_subset) > 1:\n",
    "    for i in range(1, len(peaks_ecg_subset)):\n",
    "      x_prev = df.loc[df.index[peaks_ecg_subset[i-1]], 'Time (s)']\n",
    "      x_curr = df.loc[df.index[peaks_ecg_subset[i]], 'Time (s)']\n",
    "      # Las distancias entre los distintos picos se restan en segundos, pero se almacenan en milisegundos en el vector\n",
    "      rr_interval = (x_curr - x_prev) * 1000\n",
    "      rr_intervals.append(rr_interval)\n",
    "\n",
    "  fig.update_layout(\n",
    "    title='Gráfico ECG (Intervalo de tiempo definido)',\n",
    "    xaxis_title='Tiempo (s)',\n",
    "    yaxis_title='Voltaje (mV)',\n",
    "    legend=dict(orientation='h', y=-0.2),\n",
    "    width=1000,  # Ajustar el ancho de la figura\n",
    "    height=600,  # Ajustar el alto de la figura\n",
    "    margin=dict(l=0, r=0, t=40, b=40)\n",
    "  )\n",
    "\n",
    "  fig.show()\n",
    "  # fig.show(renderer=\"browser\")\n",
    "\n",
    "  # Retornar la cantidad de picos y las distancias entre los picos\n",
    "  return len(peaks_ecg_subset), rr_intervals, peaks_ecg_subset\n",
    "\n",
    "def IBI_BVP_interval(df_subset, tiempo_ini, tiempo_fin,prominence_BVP,height_BVP,distance_BVP):\n",
    "  # Seleccionar las muestras que se deseen graficar (entre tiempo inicial y tiempo final en '# de muestra')\n",
    "  df = df_subset[(df_subset['Time (s)'] >= tiempo_ini) & (df_subset['Time (s)'] <= tiempo_fin)]\n",
    "\n",
    "  # Encontrar los picos en el subconjunto de datos\n",
    "  peaks_bvp_subset, _ = find_peaks(df['BVP_filtered_minmax'], prominence=prominence_BVP, height=np.max(df['BVP_filtered_minmax']) * height_BVP, distance=distance_BVP)\n",
    "\n",
    "  # Crear la figura con Plotly\n",
    "  fig = go.Figure()\n",
    "\n",
    "  # Graficar las muestras de BVP en el rango seleccionado\n",
    "  fig.add_trace(go.Scatter(x=df['Time (s)'], y=df['BVP_filtered_minmax'], mode='lines', name='Tiempo (s) vs BVP', line=dict(width=0.5)))\n",
    "  fig.add_trace(go.Scatter(x=df['Time (s)'].iloc[peaks_bvp_subset], y=df['BVP_filtered_minmax'].iloc[peaks_bvp_subset], mode='markers', name='Picos más altos', marker=dict(size=5)))\n",
    "\n",
    "  # Calcular y graficar las distancias entre todos los picos\n",
    "  rr_intervals = []\n",
    "  if len(peaks_bvp_subset) > 1:\n",
    "    for i in range(1, len(peaks_bvp_subset)):\n",
    "      x_prev = df.loc[df.index[peaks_bvp_subset[i-1]], 'Time (s)']\n",
    "      x_curr = df.loc[df.index[peaks_bvp_subset[i]], 'Time (s)']\n",
    "      # Las distancias entre los distintos picos se restan en segundos, pero se almacenan en milisegundos en el vector\n",
    "      rr_interval = (x_curr - x_prev) * 1000\n",
    "      rr_intervals.append(rr_interval)\n",
    "\n",
    "  fig.update_layout(\n",
    "    title='Gráfico BVP (Intervalo de tiempo)',\n",
    "    xaxis_title='Tiempo (s)',\n",
    "    yaxis_title='BVP',\n",
    "    legend=dict(orientation='h', y=-0.2),\n",
    "    width=1000,  # Ajustar el ancho de la figura\n",
    "    height=600,  # Ajustar el alto de la figura\n",
    "    margin=dict(l=0, r=0, t=40, b=40)\n",
    "  )\n",
    "\n",
    "  fig.show()\n",
    "\n",
    "  # Retornar la cantidad de picos y las distancias entre los picos\n",
    "  return len(peaks_bvp_subset), rr_intervals, peaks_bvp_subset\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "#Tratamiento de datos atipicos %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "def tratamiento_datos_atipicos_RR(df_datos_atipicos):\n",
    "  # Convertir a un DataFrame para facilitar el manejo\n",
    "  df = pd.DataFrame(df_datos_atipicos, columns=['Intervalo_RR'])\n",
    "\n",
    "  # Calcular el IQR\n",
    "  Q1 = df['Intervalo_RR'].quantile(0.25)  # Primer cuartil\n",
    "  Q3 = df['Intervalo_RR'].quantile(0.75)  # Tercer cuartil\n",
    "  IQR = Q3 - Q1  # Rango intercuartílico\n",
    "\n",
    "  # Definir límites para detectar outliers\n",
    "  limite_inferior = Q1 - 2 * IQR\n",
    "  limite_superior = Q3 + 2 * IQR\n",
    "\n",
    "  # Identificar outliers\n",
    "  outliers = df[(df['Intervalo_RR'] < limite_inferior) | (df['Intervalo_RR'] > limite_superior)]\n",
    "\n",
    "  print(\"Outliers detectados:\")\n",
    "  print(outliers)\n",
    "\n",
    "  # Calcular la mediana para reemplazar los outliers\n",
    "  mediana = df['Intervalo_RR'].median()\n",
    "\n",
    "  # Reemplazar los outliers con la mediana\n",
    "  df['Intervalo_RR'] = np.where((df['Intervalo_RR'] < limite_inferior) | (df['Intervalo_RR'] > limite_superior), mediana, df['Intervalo_RR'])\n",
    "\n",
    "  # print(\"\\nDataFrame después de tratar los outliers:\")\n",
    "  # print(df)\n",
    "  return df\n",
    "\n",
    "def tratamiento_datos_atipicos_IBI(df_datos_atipicos):\n",
    "    # Convertir a un DataFrame para facilitar el manejo\n",
    "    df = pd.DataFrame(df_datos_atipicos, columns=['Intervalo_IBI'])\n",
    "\n",
    "    # Calcular el IQR\n",
    "    Q1 = df['Intervalo_IBI'].quantile(0.25)  # Primer cuartil\n",
    "    Q3 = df['Intervalo_IBI'].quantile(0.75)  # Tercer cuartil\n",
    "    IQR = Q3 - Q1  # Rango intercuartílico\n",
    "\n",
    "    # Definir límites para detectar outliers\n",
    "    limite_inferior = Q1 - 2.5 * IQR\n",
    "    limite_superior = Q3 + 2.5 * IQR\n",
    "\n",
    "    # Identificar outliers\n",
    "    outliers = df[(df['Intervalo_IBI'] < limite_inferior) | (df['Intervalo_IBI'] > limite_superior)]\n",
    "\n",
    "    print(\"Outliers detectados:\")\n",
    "    print(outliers)\n",
    "\n",
    "    # Calcular la mediana para reemplazar los outliers\n",
    "    mediana = df['Intervalo_IBI'].median()\n",
    "\n",
    "    # Reemplazar los outliers con la mediana\n",
    "    df['Intervalo_IBI'] = np.where((df['Intervalo_IBI'] < limite_inferior) | (df['Intervalo_IBI'] > limite_superior), mediana, df['Intervalo_IBI'])\n",
    "\n",
    "    # print(\"\\nDataFrame después de tratar los outliers:\")\n",
    "    # print(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "#Calculo de parametros RR, IBI %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "def parametros_RR(df_procesada_1, tiempo_inicial, tiempo_final, prominence, height, distance):\n",
    "\n",
    "  tiempo_intervalo = tiempo_final - tiempo_inicial\n",
    "\n",
    "  # Picos, cálculo de parámetros y graficación de picos\n",
    "  cantidad_picos, distancias, data_ecg_peaks = RR_interval(df_procesada_1, tiempo_inicial, tiempo_final, prominence, height, distance)\n",
    "  \n",
    "  # Convertir distancias a un DataFrame si no lo es\n",
    "  distancias = pd.DataFrame(distancias, columns=['Intervalo_RR'])\n",
    "  \n",
    "  # Tratar datos atípicos\n",
    "  distancias = tratamiento_datos_atipicos_RR(distancias)\n",
    "  \n",
    "  print(f\"Cantidad de picos: {cantidad_picos}\")\n",
    "  print(tiempo_intervalo)\n",
    "\n",
    "  # Calculo Heart rate (se transforma de segundos a minutos)\n",
    "  heart_rate = cantidad_picos / (tiempo_intervalo * (1 / 60))\n",
    "  print(f\"HR (picos/minutos): {heart_rate}\")\n",
    "\n",
    "  # Valor promedio RR\n",
    "  RR_prom = distancias['Intervalo_RR'].mean()\n",
    "  print(f\"RR promedio (milisegundos): {RR_prom}\")\n",
    "\n",
    "  # SDNN\n",
    "  RR_sdnn = distancias['Intervalo_RR'].std()\n",
    "  print(f\"RR sdnn (milisegundos): {RR_sdnn}\")\n",
    "\n",
    "  # RMSDD\n",
    "  RR_rmssd = np.sqrt(np.mean(np.square(np.diff(distancias['Intervalo_RR']))))\n",
    "  print(f\"RR rmssd (milisegundos): {RR_rmssd}\")\n",
    "\n",
    "  # pNN50%\n",
    "  # Calcular las diferencias entre elementos consecutivos\n",
    "  RR_differences_ms = np.diff(distancias['Intervalo_RR'])\n",
    "  # Obtener los valores absolutos\n",
    "  RR_differences_absolute_values = np.abs(RR_differences_ms)\n",
    "  # Contar la cantidad de valores mayores a 50 milisegundos\n",
    "  RR_mayores_50 = np.sum(RR_differences_absolute_values > 50)\n",
    "  # Contar el número de elementos en el vector\n",
    "  Total_number_differences = len(RR_differences_absolute_values)\n",
    "  # Calculo del PNN50%\n",
    "  RR_pnn50 = (RR_mayores_50 / Total_number_differences) * 100\n",
    "  \n",
    "  print(f\"Cantidad de valores mayores a 50 ms: {RR_mayores_50}\")\n",
    "  print(f\"Cantidad de valores diferenciales: {Total_number_differences}\")\n",
    "  print(f\"PNN50% (%): {RR_pnn50}\")\n",
    "\n",
    "  # SDSD\n",
    "  RR_sdsd = np.std(RR_differences_absolute_values)\n",
    "  print(f\"RR sdsd (milisegundos): {RR_sdsd}\")\n",
    "\n",
    "  return heart_rate, RR_sdnn, RR_rmssd, RR_sdsd, RR_pnn50, data_ecg_peaks\n",
    "\n",
    "\n",
    "def parametros_IBI(df_procesada_1, tiempo_inicial_2, tiempo_final_2, prominence_BVP, height_BVP, distance_BVP):\n",
    "\n",
    "  time_2 = tiempo_final_2 - tiempo_inicial_2\n",
    "\n",
    "  # Picos, cálculo de parámetros y graficación de picos\n",
    "  cantidad_picos_2, distancias_2, data_BVP_peaks = IBI_BVP_interval(df_procesada_1, tiempo_inicial_2, tiempo_final_2, prominence_BVP, height_BVP, distance_BVP)\n",
    "\n",
    "  # Convertir distancias a un DataFrame si no lo es\n",
    "  distancias_2 = pd.DataFrame(distancias_2, columns=['Intervalo_IBI'])\n",
    "\n",
    "  # Tratar datos atípicos\n",
    "  distancias_2 = tratamiento_datos_atipicos_IBI(distancias_2)\n",
    "\n",
    "  print(f\"Cantidad de picos: {cantidad_picos_2}\")\n",
    "  print(time_2)\n",
    "\n",
    "  # Calculo Heart rate (se transforma de segundos a minutos)\n",
    "  heart_rate_2 = cantidad_picos_2 / (time_2 * (1 / 60))\n",
    "  print(f\"HR (picos/minutos): {heart_rate_2}\")\n",
    "\n",
    "  # Valor promedio IBI\n",
    "  IBI_prom = distancias_2['Intervalo_IBI'].mean()\n",
    "  print(f\"IBI promedio (milisegundos): {IBI_prom}\")\n",
    "\n",
    "  # SDNN\n",
    "  IBI_sdnn = distancias_2['Intervalo_IBI'].std()\n",
    "  print(f\"IBI sdnn (milisegundos): {IBI_sdnn}\")\n",
    "\n",
    "  # RMSDD\n",
    "  IBI_rmssd = np.sqrt(np.mean(np.square(np.diff(distancias_2['Intervalo_IBI']))))\n",
    "  print(f\"IBI rmssd (milisegundos): {IBI_rmssd}\")\n",
    "\n",
    "  # pNN50%\n",
    "  # Calcular las diferencias entre elementos consecutivos\n",
    "  IBI_differences_ms = np.diff(distancias_2['Intervalo_IBI'])\n",
    "  # Obtener los valores absolutos\n",
    "  IBI_differences_absolute_values = np.abs(IBI_differences_ms)\n",
    "  # Contar la cantidad de valores mayores a 50 milisegundos\n",
    "  IBI_mayores_50 = np.sum(IBI_differences_absolute_values > 50)\n",
    "  # Contar el número de elementos en el vector\n",
    "  Total_number_differences_IBI = len(IBI_differences_absolute_values)\n",
    "  # Calculo del PNN50%\n",
    "  IBI_pnn50 = (IBI_mayores_50 / Total_number_differences_IBI) * 100\n",
    "\n",
    "  print(f\"Cantidad de valores mayores a 50 ms: {IBI_mayores_50}\")\n",
    "  print(f\"Cantidad de valores diferenciales: {Total_number_differences_IBI}\")\n",
    "  print(f\"PNN50% (%): {IBI_pnn50}\")\n",
    "\n",
    "  # SDSD\n",
    "  IBI_sdsd = np.std(IBI_differences_absolute_values)\n",
    "  print(f\"IBI sdsd (milisegundos): {IBI_sdsd}\")\n",
    "\n",
    "  return heart_rate_2, IBI_sdnn, IBI_rmssd, IBI_sdsd, IBI_pnn50, data_BVP_peaks\n",
    "\n",
    "def RR_Correlated_intervals(peaks_ecg_ms, peaks_bvp_ms):\n",
    "  # Crear una lista para almacenar los tiempos de ECG y BVP correlacionados\n",
    "  correlated_data = []\n",
    "\n",
    "  # Iterar sobre los picos detectados en ECG\n",
    "  for i in range(len(peaks_ecg_ms)):\n",
    "    ecg_time = peaks_ecg_ms[i]\n",
    "\n",
    "    # Encontrar el punto en BVP que cumple con la condición\n",
    "    found_bvp_value = False\n",
    "    for j in range(len(peaks_bvp_ms)):\n",
    "      bvp_time = peaks_bvp_ms[j]\n",
    "\n",
    "      # Verificar si el punto de BVP está dentro del rango del punto de ECG actual y el siguiente punto de ECG\n",
    "      if ecg_time <= bvp_time <= (ecg_time if i == len(peaks_ecg_ms) - 1 else peaks_ecg_ms[i + 1]):\n",
    "        # Agregar tiempos al DataFrame de datos correlacionados\n",
    "        correlated_data.append({'ECG_PAT': ecg_time, 'BVP_PAT': bvp_time})\n",
    "        found_bvp_value = True\n",
    "        break\n",
    "\n",
    "    # Si no se encuentra un punto de BVP que cumpla con la condición, asignar un valor de cero\n",
    "    if not found_bvp_value:\n",
    "      correlated_data.append({'ECG_PAT': ecg_time, 'BVP_PAT': 0})\n",
    "\n",
    "  # Convertir la lista de datos correlacionados en un DataFrame\n",
    "  df_correlated = pd.DataFrame(correlated_data)\n",
    "\n",
    "  # Retornar el nuevo DataFrame con los tiempos de ECG y BVP correlacionados\n",
    "  return df_correlated\n",
    "\n",
    "\n",
    "def correlaciones_faltantes(df):\n",
    "  # Calcular la media de los valores no negativos\n",
    "  mean_value = df[df['PAT'] >= 0]['PAT'].mean()\n",
    "\n",
    "  # Reemplazar los valores negativos con la media calculada\n",
    "  df['PAT'] = df['PAT'].apply(lambda x: mean_value if x < 0 else x)\n",
    "  return df\n",
    "\n",
    "def parametros_PAT(df_2):\n",
    "   \n",
    "  # Calcular el promedio de la columna 'PAT'\n",
    "  PAT_mean = df_2['PAT'].mean()\n",
    "\n",
    "  # Calcular la desviación estándar de la columna 'PAT'\n",
    "  desviacion_pat = df_2['PAT'].std()\n",
    "\n",
    "  # Calcular la diferencia entre valores consecutivos de la columna 'PAT'\n",
    "  diferencias = df_2['PAT'].diff().to_numpy()\n",
    "\n",
    "  # Aplicar el valor absoluto a las diferencias\n",
    "  diferencias_abs = np.abs(diferencias)\n",
    "\n",
    "  # Filtrar los valores NaN\n",
    "  diferencias_abs_sin_nan = diferencias_abs[~np.isnan(diferencias_abs)]\n",
    "\n",
    "  # Calcular la desviación estándar de PATV\n",
    "  # Calcular la desviación estándar y la varianza\n",
    "  desviacion_estandar = np.std(diferencias_abs_sin_nan)\n",
    "  varianza = np.sqrt(np.mean(np.square(diferencias_abs_sin_nan)))\n",
    "\n",
    "  return PAT_mean, desviacion_pat, desviacion_estandar, varianza\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rango de tiempo de la toma de muestras (segundos): 303.449\n"
     ]
    }
   ],
   "source": [
    "#///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "#Implementacion del codigo--------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Lectura de los datos----------------------------------------------------------------------\n",
    "ruta_archivo = \"C:/Users/estalin.ulco/Downloads/datos_TIC/TIC/datos_entrenamiento/Student_6_Stressor.txt\"\n",
    "df=lectura_datos(ruta_archivo)\n",
    "df=convertir_columnas_a_enteros(df)\n",
    "df, tiempo_maximo_data=convertir_muestras_a_tiempo(df)\n",
    "\n",
    "#Normalizacion de los datos-------------------------------------------------------------------------------------------------\n",
    "# Implementacion de la normalizacion\n",
    "df_procesada_1=normalizar_datos(df)\n",
    "#print(df_procesada_1)\n",
    "\n",
    "#Filtrado de los datos----------------------------------------------------------------------\n",
    "\n",
    "# parametros de los filtros\n",
    "# df -> dataframe con datos a trabajar\n",
    "# order -> orden de transicion, valores altos -> se atenuan con mas rapidez las frecuencias \n",
    "# cutoff_high -> frecuencia de corte del filtro pasa alto\n",
    "# cutoff_low -> frecuencia de corte del filtro pasa bajo\n",
    "order = 5\n",
    "cutoff_high = 0.7  # frecuencia de corte, ajusta este valor según sea necesario\n",
    "cutoff_low = 30  # frecuencia de corte del filtro pasa bajo\n",
    "\n",
    "df_procesada_0_ECG = senial_ECG_pam_tompkins(df_procesada_1, order, cutoff_high, cutoff_low)\n",
    "df_procesada_0_BVP= senial_BVP_paso_bajo(df_procesada_1, order ,cutoff_low)\n",
    "\n",
    "#Union de los dataframes normalizados y limpiados\n",
    "df_procesada_0=pd.merge(df_procesada_0_BVP, df_procesada_0_ECG, on='Time (s)')\n",
    "# print(df_procesada_0)\n",
    "\n",
    "#Rangos de tiempo para hacer las graficas -----------------------------------------------------------------------------------\n",
    "\n",
    "#Tiempo maximo de graficacion varia segun el dataset que se cargue Time (s)\n",
    "print(f\"Rango de tiempo de la toma de muestras (segundos): {tiempo_maximo_data}\")\n",
    "\n",
    "\n",
    "# CALCULO DE PARAMTROS - CREACION DE DATAFRAMES CON DATOS--------------------------------------------------------------------\n",
    "\n",
    "#Picos señales ECG y BVP-----------------------------------------------------------------------------------------------------\n",
    "#Ingresar el estado de estres 0 para no estresado y 1 para estresado, dependiendo del dataset:\n",
    "est_estres = 1\n",
    "\n",
    "#Intervalo de tiempo en el cual se requier realizar los calculos de los parametros RR, IBI y PAT----------------------------\n",
    "tiempo_inicial = 0\n",
    "tiempo_final = tiempo_maximo_data\n",
    "\n",
    "#Configuracion de paramaetros para detectar PICOS ECG:\n",
    "prominence_ECG=0.01\n",
    "height_ECG=0.25\n",
    "distance_ECG=250\n",
    "\n",
    "#Configuracion de paramaetros para detectar PICOS BVP:\n",
    "prominence_BVP=0.01\n",
    "height_BVP=0.2\n",
    "distance_BVP=200\n",
    "\n",
    "#ECG calculo de parametros-----------------------------------------------------------------------------------\n",
    "# Llamar a la función y obtener los valores devueltos\n",
    "heart_rate_ECG, RR_sdnn, RR_rmssd, RR_sdsd, RR_pnn50, picos_ecg = parametros_RR(df_procesada_0,tiempo_inicial, tiempo_final,prominence_ECG,height_ECG,distance_ECG)\n",
    "# Crear una matriz numpy con los valores\n",
    "RR_matrix = np.array([heart_rate_ECG, RR_sdnn, RR_rmssd, RR_sdsd, RR_pnn50])\n",
    "\n",
    "#BVP calculo de parametros----------------------------------------------------------------------------------\n",
    "# Llamar a la función y obtener los valores devueltos\n",
    "heart_rate_BVP, IBI_sdnn, IBI_rmssd, IBI_sdsd, IBI_pnn50, picos_bvp = parametros_IBI(df_procesada_0,tiempo_inicial, tiempo_final,prominence_BVP,height_BVP,distance_BVP)\n",
    "# Crear una matriz numpy con los valores\n",
    "BVP_matrix = np.array([heart_rate_BVP, IBI_sdnn, IBI_rmssd, IBI_sdsd, IBI_pnn50])\n",
    "\n",
    "#PAT calculo de parametros----------------------------------------------------------------------------------\n",
    "#Relacion de los picos ECG con sus respectivos picos BVP\n",
    "# Llamada a la función para obtener el nuevo DataFrame con los tiempos de ECG y BVP correlacionados\n",
    "df_correlated = RR_Correlated_intervals(picos_ecg, picos_bvp)\n",
    "# print(df_correlated)\n",
    "\n",
    "#Calcular el PAT entre ECG y BVP\n",
    "# Crear un DataFrame para 'PAT' que almacena la diferencia entre 'BVP_PAT' y 'ECG_PAT'\n",
    "df_PAT = pd.DataFrame()\n",
    "df_PAT['PAT'] = df_correlated['BVP_PAT'] - df_correlated['ECG_PAT']\n",
    "df_PAT=correlaciones_faltantes(df_PAT)\n",
    "\n",
    "# Llamar a la función y obtener los valores devueltos\n",
    "PAT_mean, desviacion_pat, desviacion_estandar, varianza = parametros_PAT(df_PAT)\n",
    "# Crear una matriz numpy con los valores\n",
    "PAT_matrix = np.array([PAT_mean, desviacion_pat, desviacion_estandar, varianza])\n",
    "print(PAT_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#DataFrame en el cual almacenar los datos de los parametros calculados---------------------------------------------------------\n",
    "# Crear un DataFrame vacío con columnas \"Nombre\" y \"Apellido\"\n",
    "df_final = pd.DataFrame(columns=[\"Estres\", \"ECG_HR\", \"ECG_SDNN\", \"ECG_RMSSD\", \"ECG_SDSD\", \"ECG_PNN50\", \"BVP_HR\", \"BVP_SDNN\", \"BVP_RMSSD\", \"BVP_SDSD\", \"BVP_PNN50\", \"PAT_MEAN\", \"PAT_SD\", \"PAT_SD_PATV\", \"PAT_RMS_PATV\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Estres     ECG_HR   ECG_SDNN  ECG_RMSSD   ECG_SDSD  ECG_PNN50     BVP_HR  \\\n",
      "0      1  67.790795  82.641367  81.987708  51.092806  54.736842  67.435869   \n",
      "\n",
      "    BVP_SDNN   BVP_RMSSD    BVP_SDSD  BVP_PNN50    PAT_MEAN     PAT_SD  \\\n",
      "0  109.29433  143.191246  100.095778  60.846561  487.348285  60.970713   \n",
      "\n",
      "   PAT_SD_PATV  PAT_RMS_PATV  \n",
      "0    52.991779     64.058088  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\estalin.ulco\\AppData\\Local\\Temp\\ipykernel_11944\\3305710447.py:21: FutureWarning:\n",
      "\n",
      "The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Crear una nueva fila de datos\n",
    "new_row = pd.DataFrame({\n",
    "    \"Estres\": [est_estres],  # Asignar un valor a la columna \"Estres\"\n",
    "    \"ECG_HR\": [RR_matrix[0]],\n",
    "    \"ECG_SDNN\": [RR_matrix[1]],\n",
    "    \"ECG_RMSSD\": [RR_matrix[2]],\n",
    "    \"ECG_SDSD\": [RR_matrix[3]],\n",
    "    \"ECG_PNN50\": [RR_matrix[4]],\n",
    "    \"BVP_HR\": [BVP_matrix[0]],\n",
    "    \"BVP_SDNN\": [BVP_matrix[1]],\n",
    "    \"BVP_RMSSD\": [BVP_matrix[2]],\n",
    "    \"BVP_SDSD\": [BVP_matrix[3]],\n",
    "    \"BVP_PNN50\": [BVP_matrix[4]],\n",
    "    \"PAT_MEAN\": [PAT_matrix[0]],\n",
    "    \"PAT_SD\": [PAT_matrix[1]],\n",
    "    \"PAT_SD_PATV\": [PAT_matrix[2]],\n",
    "    \"PAT_RMS_PATV\": [PAT_matrix[3]]\n",
    "})\n",
    "\n",
    "# Agregar la nueva fila al DataFrame usando pd.concat\n",
    "df_final = pd.concat([df_final, new_row], ignore_index=True)\n",
    "\n",
    "print(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame guardado en 'df_final.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Guardar el DataFrame en un archivo Excel\n",
    "df_final.to_excel('Datos_completos_estadísticos_estalin_ulco.xlsx', index=False, engine='openpyxl')\n",
    "\n",
    "print(\"DataFrame guardado en 'df_final.xlsx'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
